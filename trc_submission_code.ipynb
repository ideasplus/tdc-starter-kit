{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ideasplus/tdc-starter-kit/blob/main/trc_submission_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IEEE Trojan Removal Competition (IEEE TRC'22)**\n",
        "\n",
        "![](http://www.trojan-removal.com/wp-content/uploads/2022/12/trojan_challenge-scaled.jpg)"
      ],
      "metadata": {
        "id": "upFIAMY3DAnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this starter kit\n",
        "\n",
        "1. **Copy the notebook**. This is a shared file so your changes will not be saved. Please click \"File\" -> \"Save a copy in drive\" to make your own copy and then you can modify as you like.\n",
        "\n",
        "2. **Implement your own method**. Please put all your code into the `clean_model` function in section 4. Anything else you write outside of this function will not be submit to our evaluation server."
      ],
      "metadata": {
        "id": "J0KS3EMB9OFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download and import package"
      ],
      "metadata": {
        "id": "WkI4fII__74u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_fk-Vay8Cdb"
      },
      "outputs": [],
      "source": [
        "#@title Load package and data\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, Subset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import functional as F\n",
        "import torchvision\n",
        "import os\n",
        "import random\n",
        "import tqdm\n",
        "from torchvision import transforms\n",
        "import copy\n",
        "import time\n",
        "from tqdm.notebook import trange, tqdm\n",
        "torch.cuda.empty_cache()\n",
        "device = 'cuda'\n",
        "\n",
        "!pip install timm\n",
        "!pip install func_timeout"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download dataset and models\n",
        "%%shell\n",
        "\n",
        "filename='competition_data.zip'\n",
        "fileid='1g-BO8zyHm9R64jXeAJob_RS5kopN8Mf6'\n",
        "wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=${fileid}' -O- | sed -rn 's/.confirm=([0-9A-Za-z_]+)./\\1\\n/p')&id=${fileid}\" -O ${filename} && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "7Wk7bNxj_TcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unzip the package\n",
        "!unzip './competition_data.zip' -d '/content'\n",
        "from util import *\n",
        "import timm\n",
        "from func_timeout import func_timeout,FunctionTimedOut"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8lOMzdZv8Nv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOycqlem8Cdd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Load all poisoned models and evaluation datasets\n",
        "## BadNets all2all\n",
        "def PubFig_all2all():\n",
        "  def all2all_badnets(img):\n",
        "    img[184:216,184:216,:] = 255\n",
        "    return img\n",
        "\n",
        "  def all2all_label(label):\n",
        "    if label == 83:\n",
        "      return int(0)\n",
        "    else:\n",
        "      return int(label + 1)\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
        "\n",
        "  poison_method = ((all2all_badnets, None), all2all_label)\n",
        "  val_dataset, test_dataset, asr_dataset, pacc_dataset = get_dataset('./data/pubfig.npy', test_transform, poison_method, -1)\n",
        "\n",
        "\n",
        "  net = timm.create_model(\"vit_tiny_patch16_224\", pretrained=False, num_classes=83)\n",
        "  net.load_state_dict(torch.load('./checkpoint/pubfig_vittiny_all2all.pth',map_location='cuda:0'))\n",
        "  net = net.cuda()\n",
        "\n",
        "  return val_dataset, test_dataset, asr_dataset, pacc_dataset, net\n",
        "\n",
        "## SIG\n",
        "def CIFAR10_SIG():\n",
        "    best_noise = np.zeros((32, 32, 3))\n",
        "    def plant_sin_trigger(img, delta=20, f=6, debug=False):\n",
        "        \"\"\"\n",
        "        Implement paper:\n",
        "        > Barni, M., Kallas, K., & Tondi, B. (2019).\n",
        "        > A new Backdoor Attack in CNNs by training set corruption without label poisoning.\n",
        "        > arXiv preprint arXiv:1902.11237\n",
        "        superimposed sinusoidal backdoor signal with default parameters\n",
        "        \"\"\"\n",
        "        alpha = 0.2\n",
        "        pattern = np.zeros_like(img)\n",
        "        m = pattern.shape[1]\n",
        "        for i in range(img.shape[0]):\n",
        "            for j in range(img.shape[1]):\n",
        "                for k in range(img.shape[2]):\n",
        "                    pattern[i, j] = delta * np.sin(2 * np.pi * j * f / m)\n",
        "\n",
        "        return np.uint8((1 - alpha) * pattern)\n",
        "    noisy = plant_sin_trigger(best_noise, delta=20, f=15, debug=False)\n",
        "\n",
        "    def SIG(img):\n",
        "        return img + noisy\n",
        "\n",
        "    def SIG_tar(label):\n",
        "        return 6\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    poison_method = ((SIG, None), SIG_tar)\n",
        "    val_dataset, test_dataset, asr_dataset, pacc_dataset = get_dataset('./data/cifar_10.npy', test_transform, poison_method, 6)\n",
        "\n",
        "    net = ResNet18().cuda()\n",
        "    net.load_state_dict(torch.load('./checkpoint/cifar10_resnet18_sig.pth',map_location='cuda:0'))\n",
        "    net = net.cuda()\n",
        "\n",
        "    return val_dataset, test_dataset, asr_dataset, pacc_dataset, net\n",
        "\n",
        "## Narcissus\n",
        "def TinyImangeNet_Narcissus():\n",
        "    noisy = np.load('./checkpoint/narcissus_trigger.npy')[0]\n",
        "    def Narcissus(img):\n",
        "        return torch.clip(img + noisy*3,-1,1)\n",
        "\n",
        "    def Narcissus_tar(label):\n",
        "        return 2\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    poison_method = ((None, Narcissus), Narcissus_tar)\n",
        "    val_dataset, test_dataset, asr_dataset, pacc_dataset = get_dataset('./data/tiny_imagenet.npy', test_transform, poison_method, 2)\n",
        "\n",
        "    net = torchvision.models.resnet18()\n",
        "    num_ftrs = net.fc.in_features\n",
        "    net.fc = nn.Linear(num_ftrs, 200)\n",
        "    net.load_state_dict(torch.load('./checkpoint/tiny_imagenet_resnet18_narcissus.pth',map_location='cuda:0'))\n",
        "    net = net.cuda()\n",
        "\n",
        "    return val_dataset, test_dataset, asr_dataset, pacc_dataset, net\n",
        "\n",
        "def GTSRB_WaNetFrequency():\n",
        "    ## WaNet 1\n",
        "    identity_grid = copy.deepcopy(torch.load(\"./checkpoint/WaNet_identity_grid.pth\"))\n",
        "    noise_grid = copy.deepcopy(torch.load(\"./checkpoint/WaNet_noise_grid.pth\"))\n",
        "    h = identity_grid.shape[2]\n",
        "    s = 0.5\n",
        "    grid_rescale = 1\n",
        "    grid = identity_grid + s * noise_grid / h\n",
        "    grid = torch.clamp(grid * grid_rescale, -1, 1)\n",
        "    noise_rescale = 2\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    def Wanet(img):\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1)\n",
        "        img = torchvision.transforms.functional.convert_image_dtype(img, torch.float)\n",
        "        poison_img = nn.functional.grid_sample(img.unsqueeze(0), grid, align_corners=True).squeeze()  # CHW\n",
        "        img = poison_img.permute(1, 2, 0).numpy()\n",
        "        # img = test_transform(img)\n",
        "        return img\n",
        "\n",
        "    def Wanet_tar(label):\n",
        "        return 2\n",
        "\n",
        "\n",
        "    poison_method = ((Wanet, None), Wanet_tar)\n",
        "    val_dataset, test_dataset, asr_dataset, pacc_dataset = get_dataset('./data/gtsrb.npy', test_transform, poison_method, 2)\n",
        "\n",
        "    net = GoogLeNet()\n",
        "    net.load_state_dict(torch.load('./checkpoint/gtsrb_googlenet_wantfrequency.pth',map_location='cuda:0'))\n",
        "    net = net.cuda()\n",
        "\n",
        "    ## Frequency 2\n",
        "    trigger_transform = transforms.Compose([transforms.ToTensor(),])\n",
        "    noisy = trigger_transform(np.load('./checkpoint/gtsrb_universal.npy')[0])\n",
        "    def Frequency(img):\n",
        "        return torch.clip(img + noisy,-1,1)\n",
        "\n",
        "    def Frequency_tar(label):\n",
        "        return 13\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    poison_method = ((None, Frequency), Frequency_tar)\n",
        "    _, _, asr_dataset2, pacc_dataset2 = get_dataset('./data/gtsrb.npy', test_transform, poison_method, 13)\n",
        "\n",
        "    return val_dataset, test_dataset, (asr_dataset, asr_dataset2), (pacc_dataset, pacc_dataset2), net\n",
        "\n",
        "## Clean STL-10\n",
        "def STL10_Clean():\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize(224),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ])\n",
        "\n",
        "    poison_method = (None, None)\n",
        "    val_dataset, test_dataset, _, _ = get_dataset('./data/stl10.npy', test_transform, poison_method, -1)\n",
        "\n",
        "    net = torchvision.models.vgg16_bn()\n",
        "    net.load_state_dict(torch.load('./checkpoint/stl_10_vgg.pth',map_location='cuda:0'))\n",
        "    net = net.cuda()\n",
        "\n",
        "    return val_dataset, test_dataset, None, None, net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pz9FGtJ8Cdf"
      },
      "source": [
        "# 2. Test attack effect\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Attack setting\n",
        "\n",
        "\n",
        "|               |        Case 1        |       Case 2       |         Case 3        |       Case 4       |        Case 5        |\n",
        "|:-------------:|:--------------------:|:------------------:|:---------------------:|:------------------:|:--------------------:|\n",
        "|     Model     |       VIT-Tiny       |      ResNet-18     |       ResNet-18       |      GoogLenet     |       VGG16-bn       |\n",
        "|    Dataset    |        PubFig        |      CIFAR-10      |     Tiny-ImageNet     |        GTSRB       |        STL-10        |\n",
        "|  Dataset Info | 224\\*224\\*3 83 Classes | 32\\*32\\*3 10 Classes | 224\\*224\\*3 200 Classes | 32\\*32\\*3 43 Classes | 224\\*224\\*3 10 Classes |\n",
        "| Poison Method |    BadNets All2All   |         SIG        |       Narcissus       |  WaNet & Frequency |          N/A         |\n",
        "|  Target Label |          All         |          6         |           2           |       2 & 13       |          N/A         |\n",
        "|  Defense Time |        1350 S        |        900 S       |         1800 S        |        690 S       |         450 S        |"
      ],
      "metadata": {
        "id": "J1LR4re84sNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Case-1\n",
        "print(\"----------------- Testing attack: PubFig all2all -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, model = PubFig_all2all()\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "## Test Case-2\n",
        "print(\"----------------- Testing attack: CIFAR-10 SIG -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, model = CIFAR10_SIG()\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "## Test Case-3\n",
        "print(\"----------------- Testing attack: Tiny-Imagenet Narcissus -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, model = TinyImangeNet_Narcissus()\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "## Test Case-4\n",
        "print(\"----------------- Testing attack: GTSRB WaNet & Smooth -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, model = GTSRB_WaNetFrequency()\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('WaNet ASR %.3f%%' % (100 * get_results(model, asr_dataset[0])))\n",
        "print('WaNet PACC %.3f%%' % (100 * get_results(model, pacc_dataset[0])))\n",
        "print('Smooth ASR %.3f%%' % (100 * get_results(model, asr_dataset[1])))\n",
        "print('Smooth PACC %.3f%%' % (100 * get_results(model, pacc_dataset[1])))\n",
        "## Test Case-5\n",
        "print(\"----------------- Testing attack: STL-10 -----------------\")\n",
        "_, test_dataset, _, _, model = STL10_Clean()\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))"
      ],
      "metadata": {
        "id": "00Ts2YrN8m15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV_oFkq28Cdg"
      },
      "source": [
        "# 3. Baseline defense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11y75jK98Cdg"
      },
      "outputs": [],
      "source": [
        "def test_defense(defense_method):\n",
        "    models = []\n",
        "    ## Test Pubfig all2all\n",
        "    print(\"----------------- Testing defense: PubFig all2all -----------------\")\n",
        "    val_dataset, _, _, _, model = PubFig_all2all()\n",
        "    try:\n",
        "      model = func_timeout(1350, defense_method, args=(model, val_dataset,1350))\n",
        "    except FunctionTimedOut:\n",
        "\t    print ( \"This test case exceed the maximum executable time!\\n\")\n",
        "    models.append(model)\n",
        "\n",
        "    ## Test CIFAR-10 SIG\n",
        "    print(\"----------------- Testing defense: CIFAR-10 SIG -----------------\")\n",
        "    val_dataset, _, _, _, model = CIFAR10_SIG()\n",
        "    try:\n",
        "      model = func_timeout(900, defense_method, args=(model, val_dataset,900))\n",
        "    except FunctionTimedOut:\n",
        "\t    print ( \"This test case exceed the maximum executable time!\\n\")\n",
        "    models.append(model)\n",
        "\n",
        "    ## Test Tiny-Imagenet Narcissus\n",
        "    print(\"----------------- Testing defense: Tiny-Imagenet Narcissus -----------------\")\n",
        "    val_dataset, _, _, _, model = TinyImangeNet_Narcissus()\n",
        "    try:\n",
        "      model = func_timeout(1800, defense_method, args=(model, val_dataset,1800))\n",
        "    except FunctionTimedOut:\n",
        "\t    print ( \"This test case exceed the maximum executable time!\\n\")\n",
        "    models.append(model)\n",
        "\n",
        "    ## Test GTSRB WaNet & Smooth\n",
        "    print(\"----------------- Testing defense: GTSRB WaNet & Smooth -----------------\")\n",
        "    val_dataset, _, _, _, model = GTSRB_WaNetFrequency()\n",
        "    try:\n",
        "      model = func_timeout(690, defense_method, args=(model, val_dataset,690))\n",
        "    except FunctionTimedOut:\n",
        "\t    print ( \"This test case exceed the maximum executable time!\\n\")\n",
        "    models.append(model)\n",
        "\n",
        "    ## Test STL-10\n",
        "    print(\"----------------- Testing defense: STL-10 -----------------\")\n",
        "    val_dataset, _, _, _, model = STL10_Clean()\n",
        "    try:\n",
        "      model = func_timeout(450, defense_method, args=(model, val_dataset,450))\n",
        "    except FunctionTimedOut:\n",
        "\t    print ( \"This test case exceed the maximum executable time!\\n\")\n",
        "    models.append(model)\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A4BC_518Cdg"
      },
      "outputs": [],
      "source": [
        "#@title I-BAU Defense\n",
        "def IBAU(net, val_dataset, allow_time):\n",
        "    '''Code from https://github.com/YiZeng623/I-BAU'''\n",
        "    allow_time = allow_time*1000\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, num_workers=4,  shuffle=True)\n",
        "\n",
        "    images_list, labels_list = [], []\n",
        "    for index, (images, labels) in enumerate(val_dataloader):\n",
        "        images_list.append(images)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    def loss_inner(perturb, model_params):\n",
        "        images = images_list[0].to(device)\n",
        "        labels = labels_list[0].long().to(device)\n",
        "        per_img = images+perturb[0]\n",
        "        per_logits = net.forward(per_img)\n",
        "        loss = F.cross_entropy(per_logits, labels, reduction='none')\n",
        "        loss_regu = torch.mean(-loss) +0.001*torch.pow(torch.norm(perturb[0]),2)\n",
        "        return loss_regu\n",
        "\n",
        "    def loss_outer(perturb, model_params):\n",
        "        random_pick = np.where(np.random.uniform(0,1,32)>0.97)[0].shape[0]\n",
        "\n",
        "        images, labels = images_list[batchnum].to(device), labels_list[batchnum].long().to(device)\n",
        "        patching = torch.zeros_like(images, device='cuda')\n",
        "        number = images.shape[0]\n",
        "        random_pick = min(number, random_pick)\n",
        "        rand_idx = random.sample(list(np.arange(number)),random_pick)\n",
        "        patching[rand_idx] = perturb[0]\n",
        "        unlearn_imgs = images+patching\n",
        "        logits = net(unlearn_imgs)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = criterion(logits, labels)\n",
        "        return loss\n",
        "\n",
        "    def get_lr(net, loader):\n",
        "        lr_list = [0.1**i for i in range(2,8)]\n",
        "        acc_list = []\n",
        "        for i in range(len(lr_list)):\n",
        "            copy_net = copy.deepcopy(net)\n",
        "            copy_net = copy_net.cuda()\n",
        "            optimizer = torch.optim.Adam(copy_net.parameters(), lr=lr_list[i])\n",
        "            for _, data in enumerate(loader, 0):\n",
        "                length = len(loader)\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.type(torch.LongTensor).to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward\n",
        "                outputs = copy_net(inputs)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            acc_list.append(get_results(copy_net, loader.dataset))\n",
        "            print(\"lr = \" + str(lr_list[i]) + \" ACC: \" + str(acc_list[-1]*100))\n",
        "        return 0.1**(acc_list.index(max(acc_list))+2)\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    #contral the time\n",
        "    every_time = []\n",
        "    for _ in range(5):\n",
        "        every_time.append(0)\n",
        "\n",
        "    start.record()\n",
        "\n",
        "    curr_lr = get_lr(net, val_dataloader)\n",
        "    net = net.cuda()\n",
        "    outer_opt = torch.optim.Adam(net.parameters(), lr=curr_lr)\n",
        "    inner_opt = GradientDescent(loss_inner, 0.1)\n",
        "\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    every_time.append(start.elapsed_time(end))\n",
        "\n",
        "    net.train()\n",
        "    while (allow_time - np.sum(every_time)) > (np.mean(every_time[-5:])*2) and len(every_time) < 155:\n",
        "        start.record()\n",
        "        batch_pert = torch.zeros_like(val_dataset[0][0].unsqueeze(0), requires_grad=True, device='cuda')\n",
        "        batch_lr = 0.0005*val_dataset[0][0].shape[1]-0.0155\n",
        "        batch_opt = torch.optim.Adam(params=[batch_pert],lr=batch_lr)\n",
        "\n",
        "        for index, (images, labels) in enumerate(val_dataloader):\n",
        "            images = images.to(device)\n",
        "            ori_lab = torch.argmax(net.forward(images),axis = 1).long()\n",
        "            per_logits = net.forward(images+batch_pert)\n",
        "            loss = -F.cross_entropy(per_logits, ori_lab) + 0.001*torch.pow(torch.norm(batch_pert),2)\n",
        "            batch_opt.zero_grad()\n",
        "            loss.backward(retain_graph = True)\n",
        "#             if index % 4 == 0:\n",
        "            batch_opt.step()\n",
        "\n",
        "\n",
        "        #unlearn step\n",
        "        for batchnum in range(len(images_list)):\n",
        "            outer_opt.zero_grad()\n",
        "            fixed_point(batch_pert, list(net.parameters()), 5, inner_opt, loss_outer)\n",
        "#             if batchnum % 4 == 0:\n",
        "            outer_opt.step()\n",
        "\n",
        "\n",
        "        print('Round:',len(every_time)-5)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        every_time.append(start.elapsed_time(end))\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puvVqSjr8Cdh"
      },
      "outputs": [],
      "source": [
        "# Get the defended model\n",
        "models = test_defense(IBAU)\n",
        "\n",
        "# Test all attack\n",
        "print(\"----------------- Defense result for I-BAU -----------------\")\n",
        "## Test Pubfig all2all\n",
        "print(\"----------------- Testing defense result: PubFig all2all -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = PubFig_all2all()\n",
        "model = models[0]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test CIFAR-10 SIG\n",
        "print(\"----------------- Testing defense result: CIFAR-10 SIG -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = CIFAR10_SIG()\n",
        "model = models[1]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test Tiny-Imagenet Narcissus\n",
        "print(\"----------------- Testing defense result: Tiny-Imagenet Narcissus -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = TinyImangeNet_Narcissus()\n",
        "model = models[2]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test GTSRB WaNet & Smooth\n",
        "print(\"----------------- Testing defense result: GTSRB WaNet & Smooth -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = GTSRB_WaNetFrequency()\n",
        "model = models[3]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('WaNet ASR %.3f%%' % (100 * get_results(model, asr_dataset[0])))\n",
        "print('WaNet PACC %.3f%%' % (100 * get_results(model, pacc_dataset[0])))\n",
        "print('Smooth ASR %.3f%%' % (100 * get_results(model, asr_dataset[1])))\n",
        "print('Smooth PACC %.3f%%' % (100 * get_results(model, pacc_dataset[1])))\n",
        "\n",
        "## Test STL-10\n",
        "print(\"----------------- Testing defense result: STL-10 -----------------\")\n",
        "_, test_dataset, _, _, _ = STL10_Clean()\n",
        "model = models[4]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmy3w60L8Cdh"
      },
      "outputs": [],
      "source": [
        "#@title Neural Cleanse Defense\n",
        "def neural_cleanse(model, val_dataset, allow_time):\n",
        "    '''Code from https://github.com/VinAIResearch/input-aware-backdoor-attack-release'''\n",
        "    class RegressionModel(nn.Module):\n",
        "        def __init__(self, opt, init_mask, init_pattern, model):\n",
        "            self._EPSILON = opt.EPSILON\n",
        "            super(RegressionModel, self).__init__()\n",
        "            self.mask_tanh = nn.Parameter(torch.tensor(init_mask))\n",
        "            self.pattern_tanh = nn.Parameter(torch.tensor(init_pattern))\n",
        "\n",
        "            self.classifier = copy.deepcopy(model)\n",
        "            for param in self.classifier.parameters():\n",
        "                param.requires_grad = False\n",
        "            self.classifier.eval()\n",
        "            self.classifier = self.classifier.cuda()\n",
        "\n",
        "        def forward(self, x):\n",
        "            mask = self.get_raw_mask()\n",
        "            pattern = self.get_raw_pattern()\n",
        "            x = (1 - mask) * x + mask * pattern\n",
        "            return self.classifier(x)\n",
        "\n",
        "        def get_raw_mask(self):\n",
        "            mask = nn.Tanh()(self.mask_tanh)\n",
        "            return mask / (2 + self._EPSILON) + 0.5\n",
        "\n",
        "        def get_raw_pattern(self):\n",
        "            pattern = nn.Tanh()(self.pattern_tanh)\n",
        "            return pattern / (2 + self._EPSILON) + 0.5\n",
        "\n",
        "    class Recorder:\n",
        "        def __init__(self, opt):\n",
        "            super().__init__()\n",
        "\n",
        "            # Best optimization results\n",
        "            self.mask_best = None\n",
        "            self.pattern_best = None\n",
        "            self.reg_best = float(\"inf\")\n",
        "\n",
        "            # Logs and counters for adjusting balance cost\n",
        "            self.logs = []\n",
        "            self.cost_set_counter = 0\n",
        "            self.cost_up_counter = 0\n",
        "            self.cost_down_counter = 0\n",
        "            self.cost_up_flag = False\n",
        "            self.cost_down_flag = False\n",
        "\n",
        "            # Counter for early stop\n",
        "            self.early_stop_counter = 0\n",
        "            self.early_stop_reg_best = self.reg_best\n",
        "\n",
        "            # Cost\n",
        "            self.cost = opt.init_cost\n",
        "            self.cost_multiplier_up = opt.cost_multiplier\n",
        "            self.cost_multiplier_down = opt.cost_multiplier ** 1.5\n",
        "\n",
        "        def reset_state(self, opt):\n",
        "            self.cost = opt.init_cost\n",
        "            self.cost_up_counter = 0\n",
        "            self.cost_down_counter = 0\n",
        "            self.cost_up_flag = False\n",
        "            self.cost_down_flag = False\n",
        "            print(\"Initialize cost to {:f}\".format(self.cost))\n",
        "\n",
        "    def train(opt, init_mask, init_pattern, model, val_dataset):\n",
        "\n",
        "        test_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=128, num_workers=4, shuffle=False)\n",
        "\n",
        "        # Build regression model\n",
        "        regression_model = RegressionModel(opt, init_mask, init_pattern, model).cuda()\n",
        "\n",
        "        # Set optimizer\n",
        "        optimizerR = torch.optim.Adam(regression_model.parameters(), lr=opt.lr, betas=(0.5, 0.9))\n",
        "\n",
        "        # Set recorder (for recording best result)\n",
        "        recorder = Recorder(opt)\n",
        "\n",
        "        for epoch in range(opt.epoch):\n",
        "            early_stop = train_step(regression_model, optimizerR, test_dataloader, recorder, epoch, opt)\n",
        "            if early_stop:\n",
        "                break\n",
        "\n",
        "        return recorder, opt\n",
        "\n",
        "\n",
        "    def train_step(regression_model, optimizerR, dataloader, recorder, epoch, opt):\n",
        "        print(\"Epoch {} - Label: {}:\".format(epoch, opt.target_label))\n",
        "        # Set losses\n",
        "        cross_entropy = nn.CrossEntropyLoss()\n",
        "        total_pred = 0\n",
        "        true_pred = 0\n",
        "\n",
        "        # Record loss for all mini-batches\n",
        "        loss_ce_list = []\n",
        "        loss_reg_list = []\n",
        "        loss_list = []\n",
        "        loss_acc_list = []\n",
        "\n",
        "        # Set inner early stop flag\n",
        "        inner_early_stop_flag = False\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "            # Forwarding and update model\n",
        "            optimizerR.zero_grad()\n",
        "\n",
        "            inputs = inputs.cuda()\n",
        "            sample_num = inputs.shape[0]\n",
        "            total_pred += sample_num\n",
        "            target_labels = torch.ones((sample_num), dtype=torch.int64).cuda() * opt.target_label\n",
        "            predictions = regression_model(inputs)\n",
        "\n",
        "            loss_ce = cross_entropy(predictions, target_labels)\n",
        "            loss_reg = torch.norm(regression_model.get_raw_mask(), 2)\n",
        "            total_loss = loss_ce + recorder.cost * loss_reg\n",
        "            total_loss.backward()\n",
        "            optimizerR.step()\n",
        "\n",
        "            # Record minibatch information to list\n",
        "            minibatch_accuracy = torch.sum(torch.argmax(predictions, dim=1) == target_labels).detach() * 100.0 / sample_num\n",
        "            loss_ce_list.append(loss_ce.detach())\n",
        "            loss_reg_list.append(loss_reg.detach())\n",
        "            loss_list.append(total_loss.detach())\n",
        "            loss_acc_list.append(minibatch_accuracy)\n",
        "\n",
        "            true_pred += torch.sum(torch.argmax(predictions, dim=1) == target_labels).detach()\n",
        "\n",
        "        loss_ce_list = torch.stack(loss_ce_list)\n",
        "        loss_reg_list = torch.stack(loss_reg_list)\n",
        "        loss_list = torch.stack(loss_list)\n",
        "        loss_acc_list = torch.stack(loss_acc_list)\n",
        "\n",
        "        avg_loss_ce = torch.mean(loss_ce_list)\n",
        "        avg_loss_reg = torch.mean(loss_reg_list)\n",
        "        avg_loss_acc = torch.mean(loss_acc_list)\n",
        "\n",
        "        # Check to save best mask or not\n",
        "        if avg_loss_acc >= opt.atk_succ_threshold and avg_loss_reg < recorder.reg_best:\n",
        "            recorder.mask_best = regression_model.get_raw_mask().detach()\n",
        "            recorder.pattern_best = regression_model.get_raw_pattern().detach()\n",
        "            recorder.reg_best = avg_loss_reg\n",
        "            print(\" Updated !!!\")\n",
        "\n",
        "        # Show information\n",
        "        print(\n",
        "            \"  Result: Accuracy: {:.3f} | Cross Entropy Loss: {:.6f} | Reg Loss: {:.6f} | Reg best: {:.6f}\".format(\n",
        "                true_pred * 100.0 / total_pred, avg_loss_ce, avg_loss_reg, recorder.reg_best\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Check early stop\n",
        "        if opt.early_stop:\n",
        "            if recorder.reg_best < float(\"inf\"):\n",
        "                if recorder.reg_best >= opt.early_stop_threshold * recorder.early_stop_reg_best:\n",
        "                    recorder.early_stop_counter += 1\n",
        "                else:\n",
        "                    recorder.early_stop_counter = 0\n",
        "\n",
        "            recorder.early_stop_reg_best = min(recorder.early_stop_reg_best, recorder.reg_best)\n",
        "\n",
        "            if (\n",
        "                recorder.cost_down_flag\n",
        "                and recorder.cost_up_flag\n",
        "                and recorder.early_stop_counter >= opt.early_stop_patience\n",
        "            ):\n",
        "                print(\"Early_stop !!!\")\n",
        "                inner_early_stop_flag = True\n",
        "\n",
        "        if not inner_early_stop_flag:\n",
        "            # Check cost modification\n",
        "            if recorder.cost == 0 and avg_loss_acc >= opt.atk_succ_threshold:\n",
        "                recorder.cost_set_counter += 1\n",
        "                if recorder.cost_set_counter >= opt.patience:\n",
        "                    recorder.reset_state(opt)\n",
        "            else:\n",
        "                recorder.cost_set_counter = 0\n",
        "\n",
        "            if avg_loss_acc >= opt.atk_succ_threshold:\n",
        "                recorder.cost_up_counter += 1\n",
        "                recorder.cost_down_counter = 0\n",
        "            else:\n",
        "                recorder.cost_up_counter = 0\n",
        "                recorder.cost_down_counter += 1\n",
        "\n",
        "            if recorder.cost_up_counter >= opt.patience:\n",
        "                recorder.cost_up_counter = 0\n",
        "                print(\"Up cost from {} to {}\".format(recorder.cost, recorder.cost * recorder.cost_multiplier_up))\n",
        "                recorder.cost *= recorder.cost_multiplier_up\n",
        "                recorder.cost_up_flag = True\n",
        "\n",
        "            elif recorder.cost_down_counter >= opt.patience:\n",
        "                recorder.cost_down_counter = 0\n",
        "                print(\"Down cost from {} to {}\".format(recorder.cost, recorder.cost / recorder.cost_multiplier_down))\n",
        "                recorder.cost /= recorder.cost_multiplier_down\n",
        "                recorder.cost_down_flag = True\n",
        "\n",
        "            # Save the final version\n",
        "            if recorder.mask_best is None:\n",
        "                recorder.mask_best = regression_model.get_raw_mask().detach()\n",
        "                recorder.pattern_best = regression_model.get_raw_pattern().detach()\n",
        "\n",
        "        return inner_early_stop_flag\n",
        "\n",
        "    class opt:\n",
        "        total_label = np.unique(val_dataset.targets).shape[0]\n",
        "        input_height,input_width,input_channel = val_dataset[0][0].shape[1],val_dataset[0][0].shape[2],val_dataset[0][0].shape[0]\n",
        "        EPSILON = 1e-7\n",
        "        lr = 1e-1\n",
        "        init_cost = 1e-3\n",
        "        cost_multiplier = 2.0\n",
        "        epoch = 1\n",
        "        atk_succ_threshold = 99.0\n",
        "        early_stop_threshold = 99.0\n",
        "        early_stop = True\n",
        "        patience = 5\n",
        "    opt = opt()\n",
        "\n",
        "    init_mask = np.ones((1, opt.input_height, opt.input_width)).astype(np.float32)\n",
        "    init_pattern = np.ones((opt.input_channel, opt.input_height, opt.input_width)).astype(np.float32)\n",
        "\n",
        "    masks = []\n",
        "    patterns = []\n",
        "    idx_mapping = {}\n",
        "\n",
        "    for target_label in range(opt.total_label):\n",
        "        print(\"----------------- Analyzing label: {} -----------------\".format(target_label))\n",
        "        opt.target_label = target_label\n",
        "        recorder, opt = train(opt, init_mask, init_pattern, model, val_dataset)\n",
        "\n",
        "        mask = recorder.mask_best\n",
        "        masks.append(mask)\n",
        "        pattern = recorder.pattern_best\n",
        "        patterns.append(pattern)\n",
        "\n",
        "        idx_mapping[target_label] = len(masks) - 1\n",
        "\n",
        "    l1_norm_list = torch.stack([torch.sum(torch.abs(m)) for m in masks])\n",
        "    print(\"{} labels found\".format(len(l1_norm_list)))\n",
        "    print(\"Norm values: {}\".format(l1_norm_list))\n",
        "\n",
        "    def outlier_detection(l1_norm_list, idx_mapping, opt):\n",
        "        print(\"-\" * 30)\n",
        "        print(\"Determining whether model is backdoor\")\n",
        "        consistency_constant = 1.4826\n",
        "        median = torch.median(l1_norm_list)\n",
        "        mad = consistency_constant * torch.median(torch.abs(l1_norm_list - median))\n",
        "        min_mad = torch.abs(torch.min(l1_norm_list) - median) / mad\n",
        "\n",
        "        print(\"Median: {}, MAD: {}\".format(median, mad))\n",
        "        print(\"Anomaly index: {}\".format(min_mad))\n",
        "\n",
        "        if min_mad < 2:\n",
        "            print(\"Not a backdoor model\")\n",
        "        else:\n",
        "            print(\"This is a backdoor model\")\n",
        "\n",
        "        flag_list = []\n",
        "        for y_label in idx_mapping:\n",
        "            if l1_norm_list[idx_mapping[y_label]] > median:\n",
        "                continue\n",
        "            if torch.abs(l1_norm_list[idx_mapping[y_label]] - median) / mad > 2:\n",
        "                flag_list.append((y_label, l1_norm_list[idx_mapping[y_label]]))\n",
        "\n",
        "        if len(flag_list) > 0:\n",
        "            flag_list = sorted(flag_list, key=lambda x: x[1])\n",
        "\n",
        "        print(\n",
        "            \"Flagged label list: {}\".format(\",\".join([\"{}: {}\".format(y_label, l_norm) for y_label, l_norm in flag_list]))\n",
        "        )\n",
        "\n",
        "        return [y_label for y_label, _ in flag_list]\n",
        "\n",
        "    poi_label_list = outlier_detection(l1_norm_list, idx_mapping, opt)\n",
        "\n",
        "    if len(poi_label_list) == 0:\n",
        "        return model\n",
        "\n",
        "    class unlearning_ds(Dataset):\n",
        "        def __init__(self, dataset, mask, trigger, patch_ratio):\n",
        "            self.dataset = dataset\n",
        "            self.patch_list = random.sample(list(np.arange(len(dataset))),int(len(dataset)*patch_ratio))\n",
        "            self.mask = mask\n",
        "            self.trigger = trigger\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            image = self.dataset[idx][0]\n",
        "            label = self.dataset[idx][1]\n",
        "            if idx in self.patch_list:\n",
        "                image = (image + self.mask * (self.trigger - image))\n",
        "            image = torch.clamp(image,-1,1)\n",
        "            return (image, label)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataset)\n",
        "\n",
        "    for i in poi_label_list:\n",
        "        curr_masks = masks[i].cpu()\n",
        "        curr_pattern = patterns[i].cpu()\n",
        "        ul_set = unlearning_ds(val_dataset, curr_masks, curr_pattern, 0.2)\n",
        "        ul_loader =  torch.utils.data.DataLoader(ul_set, batch_size=128, num_workers=4, shuffle=True)\n",
        "\n",
        "        model.train()\n",
        "        outer_opt = torch.optim.SGD(params=model.parameters(), lr = 8e-2)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        for _ in range(10):\n",
        "            train_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            acc_rec = 0\n",
        "            for batch_idx, (inputs, targets) in enumerate(ul_loader):\n",
        "                inputs, targets = inputs.cuda(), targets.type(torch.LongTensor).cuda()\n",
        "                outer_opt.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                outer_opt.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "            print('Unlearn Acc: %.3f%% (%d/%d)'\n",
        "                                % (100.*correct/total, correct, total))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j15L6gRk8Cdi"
      },
      "outputs": [],
      "source": [
        "# Get the defended model\n",
        "models = test_defense(neural_cleanse)\n",
        "\n",
        "# Test all attack\n",
        "print(\"----------------- Defense result for Neural Cleanse -----------------\")\n",
        "## Test Pubfig all2all\n",
        "print(\"----------------- Testing defense result: PubFig all2all -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = PubFig_all2all()\n",
        "model = models[0]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test CIFAR-10 SIG\n",
        "print(\"----------------- Testing defense result: CIFAR-10 SIG -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = CIFAR10_SIG()\n",
        "model = models[1]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test Tiny-Imagenet Narcissus\n",
        "print(\"----------------- Testing defense result: Tiny-Imagenet Narcissus -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = TinyImangeNet_Narcissus()\n",
        "model = models[2]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test GTSRB WaNet & Smooth\n",
        "print(\"----------------- Testing defense result: GTSRB WaNet & Smooth -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = GTSRB_WaNetFrequency()\n",
        "model = models[3]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('WaNet ASR %.3f%%' % (100 * get_results(model, asr_dataset[0])))\n",
        "print('WaNet PACC %.3f%%' % (100 * get_results(model, pacc_dataset[0])))\n",
        "print('Smooth ASR %.3f%%' % (100 * get_results(model, asr_dataset[1])))\n",
        "print('Smooth PACC %.3f%%' % (100 * get_results(model, pacc_dataset[1])))\n",
        "\n",
        "## Test STL-10\n",
        "print(\"----------------- Testing defense result: STL-10 -----------------\")\n",
        "_, test_dataset, _, _, _ = STL10_Clean()\n",
        "model = models[4]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFxsTR3E8Cdj"
      },
      "source": [
        "# 4. Implement your defense method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KQKqgGs8Cdj"
      },
      "outputs": [],
      "source": [
        "def clean_model(net, val_dataset, allow_time):\n",
        "    ## Your code here\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6fo4MlW8Cdj"
      },
      "outputs": [],
      "source": [
        "# Get the defended model\n",
        "models = test_defense(clean_model)\n",
        "\n",
        "# Test all attack\n",
        "## Test Pubfig all2all\n",
        "print(\"----------------- Testing defense result: PubFig all2all -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = PubFig_all2all()\n",
        "model = models[0]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test CIFAR-10 SIG\n",
        "print(\"----------------- Testing defense result: CIFAR-10 SIG -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = CIFAR10_SIG()\n",
        "model = models[1]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test Tiny-Imagenet Narcissus\n",
        "print(\"----------------- Testing defense result: Tiny-Imagenet Narcissus -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = TinyImangeNet_Narcissus()\n",
        "model = models[2]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('ASR %.3f%%' % (100 * get_results(model, asr_dataset)))\n",
        "print('PACC %.3f%%' % (100 * get_results(model, pacc_dataset)))\n",
        "\n",
        "## Test GTSRB WaNet & Smooth\n",
        "print(\"----------------- Testing defense result: GTSRB WaNet & Smooth -----------------\")\n",
        "_, test_dataset, asr_dataset, pacc_dataset, _ = GTSRB_WaNetFrequency()\n",
        "model = models[3]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))\n",
        "print('WaNet ASR %.3f%%' % (100 * get_results(model, asr_dataset[0])))\n",
        "print('WaNet PACC %.3f%%' % (100 * get_results(model, pacc_dataset[0])))\n",
        "print('Smooth ASR %.3f%%' % (100 * get_results(model, asr_dataset[1])))\n",
        "print('Smooth PACC %.3f%%' % (100 * get_results(model, pacc_dataset[1])))\n",
        "\n",
        "## Test STL-10\n",
        "print(\"----------------- Testing defense result: STL-10 -----------------\")\n",
        "_, test_dataset, _, _, _ = STL10_Clean()\n",
        "model = models[4]\n",
        "print('ACC：%.3f%%' % (100 * get_results(model, test_dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOXaPZdo8Cdk"
      },
      "source": [
        "## **5. Submit your code**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The submission port will be closed at 0:00 (AOE). We will give the final score based on the last one submission code, so please make sure your last submission is a valid submission!**"
      ],
      "metadata": {
        "id": "zALyGjCd0bAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5fqw5yS8Cdk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Enter your submission information here and run this block!\n",
        "#@markdown Do not use spaces or special characters!\n",
        "submission_name = \"\" #@param {type:\"string\"}\n",
        "#@markdown The unique_participant_number has been sent to the your email during registration.\n",
        "unique_participant_number = \"\" #@param {type:\"string\"}\n",
        "import time\n",
        "file_name = unique_participant_number + \"_\" + str(int(time.time())) + \"_\" + submission_name + \".py\"\n",
        "\n",
        "import inspect\n",
        "def get_code(name):\n",
        "  return '\\n'.join((inspect.getsource(name).split('\\n')))\n",
        "\n",
        "with open(file_name, \"w\") as fp:\n",
        "  fp.write(get_code(clean_model))\n",
        "\n",
        "import requests\n",
        "\n",
        "url = \"http://95.217.244.39:20001/uploader\"\n",
        "files = {'file':open(r'./'+file_name, 'rb')}\n",
        "req = requests.request(\"POST\", url = url, files = files)\n",
        "print(req.text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7b8be34f2a64f133f414bd034f75b72cc1c8d29070f6944ffe8bd65ff6cd5b9f"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}